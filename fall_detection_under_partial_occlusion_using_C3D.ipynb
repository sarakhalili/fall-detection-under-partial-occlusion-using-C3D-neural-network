{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "fall detection under partial occlusion using C3D.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "mYOCvdxvBTv4",
        "nnmM-qyx5dyu",
        "z8quWyUypuB4",
        "zMvnAFLDqB4n",
        "TLE8954da-Yv"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "f6Ube6T7NfaY",
        "outputId": "27494872-bc8e-4573-a363-12b826e94fce"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnGT4Xs3OsJt",
        "outputId": "0d2668a2-acbe-473c-cafe-08fcb867c6f9"
      },
      "source": [
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYOCvdxvBTv4"
      },
      "source": [
        "#resize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjibylRwBWNY"
      },
      "source": [
        "# this code tries to convert the fall and not-fall videos to the desired form which is 20*15-sized-frame videos, then it saves the new movies in another folder\n",
        "\n",
        "import numpy as np \n",
        "import cv2\n",
        "import os; import glob\n",
        "import re\n",
        "def atoi(text):\n",
        "    return int(text) if text.isdigit() else text\n",
        "\n",
        "def natural_keys(text):\n",
        "    return [ atoi(c) for c in re.split(r'(\\d+)', text) ]\n",
        "\n",
        "def resize(first_folder,target_folder):\n",
        "\n",
        "       \tfourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
        "        \n",
        "       \tvid_num=0\n",
        "       \n",
        "        videos = glob.glob(first_folder+\"/*.avi\")     #reading videos of the first folder\n",
        "        videos.sort(key=natural_keys)\n",
        "        for i,vid in (enumerate(videos)):\n",
        "           vid_num+=1           \n",
        "           cap = cv2.VideoCapture(vid)\n",
        "           num_frame = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "           framerate = cap.get(cv2.CAP_PROP_FPS)\n",
        "           out = cv2.VideoWriter('{}/video_{}.avi'.format(target_folder,vid_num),fourcc, framerate, (112,112),True)   #creating the resized video\n",
        "           for i in range(0, num_frame+1):\n",
        "               ret, frame=cap.read()\n",
        "               if ret==True:\n",
        "                   if i==0:\n",
        "                        r,c,_ = frame.shape # here the biggest square in the middle of frame is cropped\n",
        "                        d = (c-r)//2\n",
        "                        sc = d\n",
        "                        ec = r+d\n",
        "                   resizedfram = cv2.resize(frame[:, sc:ec], (112,112), interpolation = cv2.INTER_AREA)\n",
        "                   out.write(resizedfram)\n",
        "               else:\n",
        "                   out.release()\n",
        "                   break\n",
        "#-------------------------resize data set with the defined function named resize which receives the lower part files path and resize the movies in the directory to 15*20 movies in the new directory path which also should be received as one of the inputs------------------------------\n",
        "# resize(r\"/content/drive/MyDrive/openpose/circle/org/Coffee_room_01\",r\"/content/drive/MyDrive/openpose/circle/original/Coffee_room_01\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilciceG7GUEC"
      },
      "source": [
        "#libs\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2-wqZSaRQbF"
      },
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv3D, MaxPooling3D, ZeroPadding3D\n",
        "\n",
        "def create_model_sequential():\n",
        "    \"\"\" Creates model object with the sequential API:\n",
        "    https://keras.io/models/sequential/\n",
        "    \"\"\"\n",
        "\n",
        "    model = Sequential()\n",
        "    input_shape = (16, 112, 112, 3)\n",
        "\n",
        "    model.add(Conv3D(64, (3, 3, 3), activation='relu',\n",
        "                     padding='same', name='conv1',\n",
        "                     input_shape=input_shape))\n",
        "    model.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2),\n",
        "                           padding='valid', name='pool1'))\n",
        "    # 2nd layer group\n",
        "    model.add(Conv3D(128, (3, 3, 3), activation='relu',\n",
        "                     padding='same', name='conv2'))\n",
        "    model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2),\n",
        "                           padding='valid', name='pool2'))\n",
        "    # 3rd layer group\n",
        "    model.add(Conv3D(256, (3, 3, 3), activation='relu',\n",
        "                     padding='same', name='conv3a'))\n",
        "    model.add(Conv3D(256, (3, 3, 3), activation='relu',\n",
        "                     padding='same', name='conv3b'))\n",
        "    model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2),\n",
        "                           padding='valid', name='pool3'))\n",
        "    # 4th layer group\n",
        "    model.add(Conv3D(512, (3, 3, 3), activation='relu',\n",
        "                     padding='same', name='conv4a'))\n",
        "    model.add(Conv3D(512, (3, 3, 3), activation='relu',\n",
        "                     padding='same', name='conv4b'))\n",
        "    model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2),\n",
        "                           padding='valid', name='pool4'))\n",
        "    # 5th layer group\n",
        "    model.add(Conv3D(512, (3, 3, 3), activation='relu',\n",
        "                     padding='same', name='conv5a'))\n",
        "    model.add(Conv3D(512, (3, 3, 3), activation='relu',\n",
        "                     padding='same', name='conv5b'))\n",
        "    model.add(ZeroPadding3D(padding=((0, 0), (0, 1), (0, 1)), name='zeropad5'))\n",
        "    model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2),\n",
        "                           padding='valid', name='pool5'))\n",
        "    model.add(Flatten(name='flat5'))\n",
        "    # FC layers group\n",
        "    model.add(Dense(4096, activation='relu', name='fc6'))\n",
        "    model.add(Dropout(.5))\n",
        "    model.add(Dense(4096, activation='relu', name='fc7'))\n",
        "    model.add(Dropout(.5))\n",
        "    model.add(Dense(487, activation='softmax', name='fc8'))\n",
        "\n",
        "    return model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuP1r-sxLpQL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "outputId": "d2c671de-be41-47e8-e170-5c50a51c995e"
      },
      "source": [
        "import numpy as np\n",
        "ar = np.array\n",
        "import cv2\n",
        "import pickle\n",
        "from sklearn import svm\n",
        "from scipy.signal import find_peaks\n",
        "import os; import glob\n",
        "from tensorflow.python.keras import layers, models\n",
        "from tensorflow.python.keras.layers import Dropout\n",
        "from tensorflow.python.keras.layers import Dense\n",
        "from keras.constraints import maxnorm\n",
        "import re\n",
        "\n",
        "def atoi(text):\n",
        "    return int(text) if text.isdigit() else text\n",
        "\n",
        "def natural_keys(text):\n",
        "    return [ atoi(c) for c in re.split(r'(\\d+)', text) ]\n",
        "\n",
        "c3d_len = 16\n",
        "\n",
        "def load_c3d():                       # defining C3D network\n",
        "\n",
        "    import tensorflow as tf\n",
        "    from tensorflow.keras.models import Sequential, Model\n",
        "    model = create_model_sequential()\n",
        "    try:\n",
        "        model.load_weights('/content/drive/MyDrive/C3D_Sport1M_weights_keras_2.2.4.h5')   #weights of the network should be downloaded \n",
        "    except OSError as err:\n",
        "        print('Check path to the model weights\\' file!\\n\\n', err)\n",
        "    except :\n",
        "        print(\"errooooor\")\n",
        "\n",
        "    C3D = Model(inputs=model.input,\n",
        "                      outputs=[ model.get_layer('fc6').output,\n",
        "                               model.get_layer('flat5').output,\n",
        "                               model.get_layer('fc7').output,\n",
        "                               model.get_layer('fc8').output])\n",
        "    CNN_FC5 = models.Model(inputs=C3D.inputs, outputs=C3D.layers[-6].output)          # defining a network only with the convolution part of the C3D network (essential for training the FCN)\n",
        "    CNN_FC5.summary()\n",
        "    del model\n",
        "    return CNN_FC5\n",
        "\n",
        "CNN_FC5=load_c3d()\n",
        "new_train=False\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-01247dbe2979>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmyCNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mmyCNN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_c3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-01247dbe2979>\u001b[0m in \u001b[0;36mload_c3d\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     C3D = Model(inputs=model.input,\n\u001b[0;32m---> 46\u001b[0;31m                       outputs=[ model.get_layer('fc6').output,\n\u001b[0m\u001b[1;32m     47\u001b[0m                                \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'flat5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                                \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fc7'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mget_layer\u001b[0;34m(self, name, index)\u001b[0m\n\u001b[1;32m   2826\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2827\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2828\u001b[0;31m       raise ValueError(f'No such layer: {name}. Existing layers are: '\n\u001b[0m\u001b[1;32m   2829\u001b[0m                        f'{list(layer.name for layer in self.layers)}.')\n\u001b[1;32m   2830\u001b[0m     raise ValueError('Provide either a layer name or layer index at '\n",
            "\u001b[0;31mValueError\u001b[0m: No such layer: fc6. Existing layers are: ['conv1', 'pool1', 'conv2', 'pool2', 'conv3a', 'conv3b', 'pool3', 'conv4a', 'conv4b', 'pool4', 'conv5a', 'conv5b', 'zeropad5', 'pool5', 'flat5', 'module_wrapper', 'module_wrapper_1', 'module_wrapper_2', 'module_wrapper_3', 'module_wrapper_4']."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD8khC8JR8ZU"
      },
      "source": [
        "from tqdm import tqdm               \n",
        "        \n",
        "        \n",
        "# ===========================\n",
        "def ext(l,c3d):                 # This function gives the result of c3d network for the given input frames\n",
        "    \n",
        "    frames16 = l[:16].reshape((1,16,112,112,3)).astype('int') -100 # .astype('int')*255\n",
        "    features_4layers = c3d.predict(frames16)\n",
        "    return features_4layers\n",
        "\n",
        "\n",
        "\n",
        "class extraction():                 # This class returns extracted features and their labels\n",
        "    def __init__(self, dataset_name, options, label,ds_addr,data_addr):\n",
        "        self.ds_addr = ds_addr;             # address of the dataset location\n",
        "        self.data_addr = data_addr;         # address of the extracted features\n",
        "        self.dataset_name = dataset_name;   # name of the folder\n",
        "        self.label = label\n",
        "        self.options = options\n",
        "        self.dsrate = options['downsample_rate']\n",
        "        self.stride = options['stride'] \n",
        "\n",
        "    def extract(self):\n",
        "        print(self.label, \"extracting ...\")\n",
        "        if self.dataset_name not in os.listdir(self.ds_addr):\n",
        "            print(\"error , folder with dataset_name was not found\\n\\n\")\n",
        "  \n",
        "            \n",
        "        f = open(self.ds_addr+ self.dataset_name+'/'+self.dataset_name+'.txt', \"r+\")\n",
        "        annot = f.readlines()                                                                 # reading lines of each text file\n",
        "        \n",
        "        self.annot=[]\n",
        "        f.close()\n",
        "        videos = glob.glob(self.ds_addr+ self.dataset_name+'/*.avi')                          #reading videos of the dataset folder\n",
        "        videos.sort(key=natural_keys)        \n",
        "        CNN_FC5=load_c3d()                                                                    # loading only the convolution part of the C3D network\n",
        "\n",
        "        fc5_n=[]; fc5_o=[]; labels_n=[]; labels_o=[];\n",
        "        for j,video in (enumerate(videos)):                                  \n",
        "\n",
        "            start=int(annot[j].split()[0])\n",
        "            end=int(annot[j].split()[1])\n",
        "            f_all=int(annot[j].split()[2])\n",
        "\n",
        "            capture = cv2.VideoCapture( video )\n",
        "            num_frame = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "            if f_all!=num_frame:\n",
        "                print(\"error, num frames\", num_frame,f_all)\n",
        "            if not capture.isOpened:\n",
        "                print('Unable to open: ' + inp)\n",
        "                exit(0)\n",
        "            \n",
        "            frame16= []\n",
        "            i=0\n",
        "            for i in range(0, num_frame):\n",
        "                ret, frame = capture.read()\n",
        "                    \n",
        "                if i% self.dsrate==0:                                     # applying downsampling to the frames\n",
        "                    frame_resized=frame\n",
        "                    frame16 += [frame_resized]\n",
        "                if frame is None:\n",
        "                    break\n",
        "        #============================================\n",
        "            frame16 = ar(frame16)\n",
        "            c3dlength = 16\n",
        "            if len(frame16)<16 :\n",
        "                print(\"error... not enough frames\")\n",
        "                continue\n",
        "            sample_number = (len(frame16)-c3dlength)// self.stride +1       # calculating the number of input segments         \n",
        "            for k in range(sample_number):\n",
        "                sc=k*self.stride; ec=k*self.stride+16                       # applying stride to the frames\n",
        "                xn = ext(frame16[sc:ec],CNN_FC5)\n",
        "                fc5_n+=[xn]\n",
        "                if start==0 and end==0:\n",
        "                  label=[0,1]   \n",
        "                elif (sc*self.dsrate)>=start and (ec*self.dsrate)<=end:\n",
        "                  label=[1,0]\n",
        "                else:\n",
        "                  label=[0,1]      \n",
        "                labels_n+=[label]\n",
        "                \n",
        "\n",
        "        self.fc5_n=fc5_n\n",
        "        print(np.shape(fc5_n))\n",
        "        np.save(self.data_addr +\"/{}_fc5.npy\".format(self.dataset_name),fc5_n)\n",
        "        np.save(self.data_addr +\"/{}_labels.npy\".format(self.dataset_name),labels_n)\n",
        "        return fc5_n,labels_n\n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extracting FC5"
      ],
      "metadata": {
        "id": "IqVlFz5A5gx3"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "s2t8dF2DfH17",
        "outputId": "acdc109f-d9ee-4689-9e58-0446350bb6bd"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA as sklearnPCA\n",
        "\n",
        "from tensorflow.python.keras import metrics\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tqdm import tqdm\n",
        "ar = np.array\n",
        "new_train=False\n",
        "# !rm -rf '/content/drive/MyDrive/full_frame/data/conv4/original/'\n",
        "# !mkdir '/content/drive/MyDrive/full_frame/data/conv4/original/'\n",
        "ds_addr= '/content/drive/MyDrive/full_frame/cross_subject/original/'\n",
        "data_addr = '/content/drive/MyDrive/full_frame/data/conv4/original/'\n",
        "options={'downsample_rate':1 , 'background_sub':False, 'stride':4}\n",
        "\n",
        "\n",
        "# datasets=[['Home_01','Home_02','Office','Office2','Coffee_room_01','ur_fall','ur_adl','Lecture_room','mine','mine_fall']]\n",
        "datasets=[['train']]\n",
        "for j,folder in (enumerate(datasets)):\n",
        "\n",
        "  for i,name in (enumerate(folder)):\n",
        "      try:\n",
        "        fc5_n=np.load(data_addr+\"/{}_fc5.npy\".format(name))\n",
        "        labels_n=np.load(data_addr+\"/{}_labels.npy\".format(name))\n",
        "        print(\"{} loaded successfully\".format(name))\n",
        "      except:\n",
        "        ds=extraction(name, options , name,ds_addr,data_addr)\n",
        "      \n",
        "        fc5_n,labels_n=ds.extract()\n",
        "        fc5_n=ar(fc5_n); labels_n=ar(labels_n)  \n",
        "      if i==0:\n",
        "        if len(fc5_n)!=0:\n",
        "          fc5n_all=fc5_n\n",
        "          labelsn_all=labels_n\n",
        "      else:\n",
        "        if len(fc5_n)!=0:\n",
        "          fc5n_all=np.concatenate((fc5n_all,fc5_n),axis=0)\n",
        "          labelsn_all=np.concatenate((labelsn_all,labels_n),axis=0)\n",
        "        \n",
        "     \n",
        "##################################################reshaping fc5 and labels_all for network ###################################\n",
        "\n",
        "fc5n_all=np.reshape(fc5n_all,(-1,2, 7, 7, 512) ) \n",
        "\n",
        "size=np.size(labelsn_all); size=int(size/2)\n",
        "labelsn_all=ar(labelsn_all)\n",
        "labelsn_all=np.reshape(labelsn_all,(size,2))\n",
        "Xntrain=fc5n_all; Yntrain=labelsn_all\n",
        "del fc5n_all,labelsn_all\n",
        "# datasets=[['Home_01','Home_02','Office','Office2','Coffee_room_01','ur_fall','ur_adl','Lecture_room','mine','mine_fall']]\n",
        "datasets=[['test']]\n",
        "for j,folder in (enumerate(datasets)):\n",
        "\n",
        "  for i,name in (enumerate(folder)):\n",
        "      try:\n",
        "        fc5_n=np.load(data_addr+\"/{}_fc5.npy\".format(name))\n",
        "        labels_n=np.load(data_addr+\"/{}_labels.npy\".format(name))\n",
        "        print(\"{} loaded successfully\".format(name))\n",
        "      except:\n",
        "        ds=extraction(name, options , name,ds_addr,data_addr)\n",
        "      \n",
        "        fc5_n,labels_n=ds.extract()\n",
        "        fc5_n=ar(fc5_n); labels_n=ar(labels_n)  \n",
        "      if i==0:\n",
        "        if len(fc5_n)!=0:\n",
        "          fc5n_all=fc5_n\n",
        "          labelsn_all=labels_n\n",
        "      else:\n",
        "        if len(fc5_n)!=0:\n",
        "          fc5n_all=np.concatenate((fc5n_all,fc5_n),axis=0)\n",
        "          labelsn_all=np.concatenate((labelsn_all,labels_n),axis=0)\n",
        "        \n",
        "     \n",
        "##################################################reshaping fc5 and labels_all for network ###################################\n",
        "fc5n_all=np.reshape(fc5n_all,(-1,2, 7, 7, 512) ) \n",
        "\n",
        "size=np.size(labelsn_all); size=int(size/2)\n",
        "labelsn_all=ar(labelsn_all)\n",
        "labelsn_all=np.reshape(labelsn_all,(size,2))\n",
        "Xnvalid=fc5n_all; Ynvalid= labelsn_all\n",
        "del fc5n_all,labelsn_all"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train extracting ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-270110c59996>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mfc5_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_addr\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/{}_fc5.npy\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mlabels_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_addr\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/{}_labels.npy\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{} loaded successfully\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/full_frame/data/conv4/original//train_labels.npy'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-270110c59996>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mds_addr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_addr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mfc5_n\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mfc5_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfc5_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mlabels_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-b330b4894547>\u001b[0m in \u001b[0;36mextract\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0msc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mxn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe16\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mec\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmyCNN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m                 \u001b[0mfc5_n\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mxn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-b330b4894547>\u001b[0m in \u001b[0;36mext\u001b[0;34m(l, c3d)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# 16 black frames with 3 channels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mframes16\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m112\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m112\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;31m# .astype('int')*255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mfeatures_4layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc3d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfeatures_4layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1704\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1705\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1706\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1708\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1362\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cluster_coordinator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1364\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    335\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mflat_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mflat_map\u001b[0;34m(self, map_func)\u001b[0m\n\u001b[1;32m   1955\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1956\u001b[0m     \"\"\"\n\u001b[0;32m-> 1957\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mFlatMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m   def interleave(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func)\u001b[0m\n\u001b[1;32m   4563\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4564\u001b[0m     self._map_func = StructuredFunctionWrapper(\n\u001b[0;32m-> 4565\u001b[0;31m         map_func, self._transformation_name(), dataset=input_dataset)\n\u001b[0m\u001b[1;32m   4566\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4567\u001b[0m       raise TypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   3710\u001b[0m     \u001b[0mresource_tracker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResourceTracker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3711\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3712\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3713\u001b[0m       \u001b[0;31m# There is no graph to add in eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3714\u001b[0m       \u001b[0madd_to_graph\u001b[0m \u001b[0;34m&=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3133\u001b[0m     \"\"\"\n\u001b[1;32m   3134\u001b[0m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0;32m-> 3135\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   3136\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3137\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3098\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3099\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3100\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3101\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3102\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3443\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3444\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3445\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3287\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3288\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3289\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3290\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3685\u001b[0m           attributes=defun_kwargs)\n\u001b[1;32m   3686\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3687\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3688\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3689\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3615\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_should_unpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3616\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3617\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3618\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0m_should_pack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3619\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_allowlisted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m   \u001b[0;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mslice_batch_indices\u001b[0;34m(indices)\u001b[0m\n\u001b[1;32m    320\u001b[0m       \"\"\"\n\u001b[1;32m    321\u001b[0m       \u001b[0mnum_in_full_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_full_batches\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m       \u001b[0mfirst_k_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnum_in_full_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m       first_k_indices = array_ops.reshape(\n\u001b[1;32m    324\u001b[0m           first_k_indices, [num_full_batches, batch_size])\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mslice\u001b[0;34m(input_, begin, size, name)\u001b[0m\n\u001b[1;32m   1105\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m   \"\"\"\n\u001b[0;32m-> 1107\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbegin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36m_slice\u001b[0;34m(input, begin, size, name)\u001b[0m\n\u001b[1;32m   9409\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9410\u001b[0m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[0;32m-> 9411\u001b[0;31m         \"Slice\", input=input, begin=begin, size=size, name=name)\n\u001b[0m\u001b[1;32m   9412\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9413\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m                 \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_ref\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m                 preferred_dtype=default_dtype)\n\u001b[0m\u001b[1;32m    523\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1566\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    337\u001b[0m                                          as_ref=False):\n\u001b[1;32m    338\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    263\u001b[0m   \"\"\"\n\u001b[1;32m    264\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 265\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    281\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[1;32m    282\u001b[0m           \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m           allow_broadcast=allow_broadcast))\n\u001b[0m\u001b[1;32m    284\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m   \u001b[0mattrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtensor_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdtype_value\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m   if dtype is not None and (not hasattr(dtype, \"base_dtype\") or\n\u001b[0;32m--> 490\u001b[0;31m                             dtype.base_dtype != numpy_dtype.base_dtype):\n\u001b[0m\u001b[1;32m    491\u001b[0m     raise TypeError(\"Incompatible types: %s vs. %s. Value is %s\" %\n\u001b[1;32m    492\u001b[0m                     (dtype, nparray.dtype, values))\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py\u001b[0m in \u001b[0;36mbase_dtype\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbase_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;34m\"\"\"Returns a non-reference `DType` based on this `DType`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_ref_dtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_INTERN_TABLE\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_type_enum\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py\u001b[0m in \u001b[0;36m_is_ref_dtype\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     68\u001b[0m   \u001b[0m__slots__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_is_ref_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;34m\"\"\"Returns `True` if this `DType` represents a reference type.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ACYOeDRmvJZ"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA as sklearnPCA\n",
        "\n",
        "from tensorflow.python.keras import metrics\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tqdm import tqdm\n",
        "ar = np.array\n",
        "!rm -rf '/content/drive/MyDrive/full_frame/data/conv4/openpose/'\n",
        "!mkdir '/content/drive/MyDrive/full_frame/data/conv4/openpose/'\n",
        "ds_addr= '/content/drive/MyDrive/full_frame/cross_subject/openpose/'\n",
        "data_addr = '/content/drive/MyDrive/full_frame/data/conv4/openpose/'\n",
        "options={'downsample_rate':1 , 'background_sub':False, 'stride':16}\n",
        "\n",
        "# datasets=[['Home_01','Home_02','Office','Office2','Coffee_room_01','ur_fall','ur_adl','Lecture_room','mine']]\n",
        "datasets=[['train']]\n",
        "for j,folder in (enumerate(datasets)):\n",
        "\n",
        "  for i,name in (enumerate(folder)):\n",
        "      try:\n",
        "        fc5_o=np.load(data_addr+\"/{}_fc5.npy\".format(name))\n",
        "        labels_o=np.load(data_addr+\"/{}_labels.npy\".format(name))\n",
        "        print(\"{} loaded successfully\".format(name))\n",
        "      except:\n",
        "        ds=extraction(name, options , name,ds_addr,data_addr)\n",
        "      \n",
        "        fc5_o,labels_o=ds.extract()\n",
        "        fc5_o=ar(fc5_o);  labels_o=ar(labels_o)     \n",
        "      if i==0:\n",
        "        if len(fc5_o!=0):\n",
        "          fc5o_all=fc5_o\n",
        "          labelso_all=labels_o\n",
        "\n",
        "      else:\n",
        "        if len(fc5_o!=0):\n",
        "          fc5o_all=np.concatenate((fc5o_all,fc5_o),axis=0)\n",
        "          labelso_all=np.concatenate((labelso_all,labels_o),axis=0)    \n",
        "##################################################reshaping fc5 and labels_all for network ###################################\n",
        "\n",
        "d1,d2,d3=np.shape(fc5o_all)\n",
        "fc5o_all=np.reshape(fc5o_all,(d1,d3)) \n",
        "\n",
        "size=np.size(labelso_all); size=int(size/2)\n",
        "labelso_all=ar(labelso_all)\n",
        "labelso_all=np.reshape(labelso_all,(size,2))\n",
        "Xotrain=fc5o_all; Yotrain= labelso_all\n",
        "del fc5o_all,labelso_all\n",
        "\n",
        "datasets=[['test']]\n",
        "for j,folder in (enumerate(datasets)):\n",
        "\n",
        "  for i,name in (enumerate(folder)):\n",
        "      try:\n",
        "        fc5_o=np.load(data_addr+\"/{}_fc5.npy\".format(name))\n",
        "        labels_o=np.load(data_addr+\"/{}_labels.npy\".format(name))\n",
        "        print(\"{} loaded successfully\".format(name))\n",
        "      except:\n",
        "        ds=extraction(name, options , name,ds_addr,data_addr)\n",
        "      \n",
        "        fc5_o,labels_o=ds.extract()\n",
        "        fc5_o=ar(fc5_o);  labels_o=ar(labels_o)     \n",
        "      if i==0:\n",
        "        if len(fc5_o!=0):\n",
        "          fc5o_all=fc5_o\n",
        "          labelso_all=labels_o\n",
        "\n",
        "      else:\n",
        "        if len(fc5_o!=0):\n",
        "          fc5o_all=np.concatenate((fc5o_all,fc5_o),axis=0)\n",
        "          labelso_all=np.concatenate((labelso_all,labels_o),axis=0)\n",
        "        \n",
        "     \n",
        "##################################################reshaping fc5 and labels_all for network ###################################\n",
        "\n",
        "d1,d2,d3=np.shape(fc5o_all)\n",
        "fc5o_all=np.reshape(fc5o_all,(d1,d3)) \n",
        "\n",
        "size=np.size(labelso_all); size=int(size/2)\n",
        "labelso_all=ar(labelso_all)\n",
        "labelso_all=np.reshape(labelso_all,(size,2))\n",
        "\n",
        "Xovalid=fc5o_all; Yovalid= labelso_all\n",
        "del fc5o_all,labelso_all\n",
        "    #####################################################################################################    \n",
        "# print(np.shape(fc5n_all),np.shape(labelsn_all),np.shape(fc5o_all),np.shape(labelso_all))\n",
        "# Xn2, Xntest, Yn2,Yntest = train_test_split(fc5n_all, labelsn_all, test_size=0.2, random_state=42 )\n",
        "# Xntrain, Xnvalid, Yntrain,Ynvalid = train_test_split(Xn2, Yn2, test_size=0.2 , random_state=42) \n",
        "# print(\"Normal Xtrain is:\",np.shape(Xntrain),\"          Normal Xvalid is:\",np.shape(Xnvalid),\"          Normal Xtest is:\",np.shape(Xntest))   \n",
        "# print(\"Normal Ytrain is:\",np.shape(Yntrain),\"          Normal Yvalid is:\",np.shape(Ynvalid),\"          Normal Ytest is:\",np.shape(Yntest))    \n",
        "# del fc5n_all,labelsn_all,Xn2,Yn2     \n",
        "    #####################################################################################################  \n",
        "\n",
        "# Xo2, Xotest, Yo2,Yotest = train_test_split(fc5o_all, labelso_all, test_size=0.2, random_state=42 )\n",
        "# Xotrain, Xovalid, Yotrain,Yovalid = train_test_split(Xo2, Yo2, test_size=0.2 , random_state=42) \n",
        "# print(\"Occluded Xtrain is:\",np.shape(Xotrain),\"          Occluded Xvalid is:\",np.shape(Xovalid),\"          Occluded Xtest is:\",np.shape(Xotest))   \n",
        "# print(\"Occluded Ytrain is:\",np.shape(Yotrain),\"          Occluded Yvalid is:\",np.shape(Yovalid),\"          Occluded Ytest is:\",np.shape(Yotest))  \n",
        "# del fc5o_all,labelso_all,Xo2,Yo2         \n",
        "    ##################################################################################################### \n",
        "\n",
        "Xtrain=np.concatenate((Xntrain,Xotrain),axis=0);  Ytrain=np.concatenate((Yntrain,Yotrain),axis=0)\n",
        "maximum=np.amax(Xtrain); Xntrain=Xntrain/maximum; Xotrain=Xotrain/maximum\n",
        "# Xtest=np.concatenate((Xntest,Xotest),axis=0);     Ytest=np.concatenate((Yntest,Yotest),axis=0)\n",
        "Xvalid=np.concatenate((Xnvalid,Xovalid),axis=0);     Yvalid=np.concatenate((Ynvalid,Yovalid),axis=0)\n",
        "maximum=np.amax(Xvalid); Xovalid=Xovalid/maximum; Xnvalid=Xnvalid/maximum; Xvalid=Xvalid/maximum\n",
        "# Xvalid, Xtest, Yvalid,Ytest = train_test_split(Xvalid, Yvalid, test_size=0.5)\n",
        "    ##################################################################################################### \n",
        "\n",
        "# fc_all=np.concatenate((fc5n_all,fc5o_all),axis=0)\n",
        "# labels_all=np.concatenate((labelsn_all,labelso_all),axis=0)\n",
        "\n",
        "# print(np.shape(fc_all),np.shape(labels_all))\n",
        "# X2, Xtest, Y2,Ytest = train_test_split(fc_all, labels_all, test_size=0.3, random_state=42 )\n",
        "# Xtrain, Xvalid, Ytrain,Yvalid = train_test_split(X2, Y2, test_size=0.2 , random_state=42) \n",
        "#     ##################################################################################################### \n",
        "\n",
        "# print(\"Xtrain is:\",np.shape(Xtrain),\"          Xvalid is:\",np.shape(Xvalid),\"          Xtest is:\",np.shape(Xtest))   \n",
        "# print(\"Ytrain is:\",np.shape(Ytrain),\"          Yvalid is:\",np.shape(Yvalid),\"          Ytest is:\",np.shape(Ytest))  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHJrmTnzZCmp"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA as sklearnPCA\n",
        "\n",
        "from tensorflow.python.keras import metrics\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tqdm import tqdm\n",
        "ar = np.array\n",
        "\n",
        "# !rm -rf '/content/drive/MyDrive/yolo/data/flatten/openpose/'\n",
        "# !mkdir '/content/drive/MyDrive/yolo/data/flatten/openpose/'\n",
        "ds_addr= '/content/drive/MyDrive/yolo/original/'\n",
        "data_addr = '/content/drive/MyDrive/yolo/data/flatten/original/'\n",
        "options={'downsample_rate':1 , 'background_sub':False, 'stride':4}\n",
        "\n",
        "\n",
        "# datasets=[['Home_01','Home_02','Office','Office2','Coffee_room_01','ur_fall','ur_adl','Lecture_room','mine','mine_fall']]\n",
        "datasets=[['mine2','mine','mine_fall']]\n",
        "for j,folder in (enumerate(datasets)):\n",
        "\n",
        "  for i,name in (enumerate(folder)):\n",
        "      try:\n",
        "        fc5_n=np.load(data_addr+\"/{}_fc5.npy\".format(name))\n",
        "        labels_n=np.load(data_addr+\"/{}_labels.npy\".format(name))\n",
        "        print(\"{} loaded successfully\".format(name))\n",
        "      except:\n",
        "        ds=extraction(name, options , name,ds_addr,data_addr)\n",
        "      \n",
        "        fc5_n,labels_n=ds.extract()\n",
        "        fc5_n=ar(fc5_n); labels_n=ar(labels_n)  \n",
        "      if i==0:\n",
        "        if len(fc5_n)!=0:\n",
        "          fc5n_all=fc5_n\n",
        "          labelsn_all=labels_n\n",
        "      else:\n",
        "        if len(fc5_n)!=0:\n",
        "          fc5n_all=np.concatenate((fc5n_all,fc5_n),axis=0)\n",
        "          labelsn_all=np.concatenate((labelsn_all,labels_n),axis=0)\n",
        "        \n",
        "     \n",
        "##################################################reshaping fc5 and labels_all for network ###################################\n",
        "d1,d2,d3=np.shape(fc5n_all)\n",
        "fc5n_all=np.reshape(fc5n_all,(d1,d3)) \n",
        "\n",
        "size=np.size(labelsn_all); size=int(size/2)\n",
        "labelsn_all=ar(labelsn_all)\n",
        "labelsn_all=np.reshape(labelsn_all,(size,2))\n",
        "Xntest=fc5n_all; Yntest=labelsn_all\n",
        "del fc5n_all,labelsn_all\n",
        "\n",
        "ds_addr= '/content/drive/MyDrive/openpose/circle/yolo_10/'\n",
        "data_addr = '/content/drive/MyDrive/yolo/data/flatten/openpose/'\n",
        "options={'downsample_rate':1 , 'background_sub':False, 'stride':16}\n",
        "\n",
        "datasets=[['mine','mine_fall']]\n",
        "for j,folder in (enumerate(datasets)):\n",
        "\n",
        "  for i,name in (enumerate(folder)):\n",
        "      try:\n",
        "        fc5_n=np.load(data_addr+\"/{}_fc5.npy\".format(name))\n",
        "        labels_n=np.load(data_addr+\"/{}_labels.npy\".format(name))\n",
        "        print(\"{} loaded successfully\".format(name))\n",
        "      except:\n",
        "        ds=extraction(name, options , name,ds_addr,data_addr)\n",
        "      \n",
        "        fc5_n,labels_n=ds.extract()\n",
        "        fc5_n=ar(fc5_n); labels_n=ar(labels_n)  \n",
        "      if i==0:\n",
        "        if len(fc5_n)!=0:\n",
        "          fc5n_all=fc5_n\n",
        "          labelsn_all=labels_n\n",
        "      else:\n",
        "        if len(fc5_n)!=0:\n",
        "          fc5n_all=np.concatenate((fc5n_all,fc5_n),axis=0)\n",
        "          labelsn_all=np.concatenate((labelsn_all,labels_n),axis=0)\n",
        "        \n",
        "     \n",
        "##################################################reshaping fc5 and labels_all for network ###################################\n",
        "d1,d2,d3=np.shape(fc5n_all)\n",
        "fc5n_all=np.reshape(fc5n_all,(d1,d3)) \n",
        "\n",
        "size=np.size(labelsn_all); size=int(size/2)\n",
        "labelsn_all=ar(labelsn_all)\n",
        "labelsn_all=np.reshape(labelsn_all,(size,2))\n",
        "Xotest=fc5n_all; Yotest= labelsn_all\n",
        "del fc5n_all,labelsn_all\n",
        "Xtest=np.concatenate((Xntest,Xotest),axis=0);     Ytest=np.concatenate((Yntest,Yotest),axis=0)\n",
        "maximum=np.amax(Xtest); Xtest=Xtest/maximum\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2pkCz2CmqMV"
      },
      "source": [
        "#training fcn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8nsvM5lCu32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4a4c170e-4236-401c-f0eb-0e4599622831"
      },
      "source": [
        "def fcn():             # creating fully connected part of C3D network for training\n",
        "\n",
        "    model = Sequential()\n",
        "    input_shape = (2, 7, 7, 512) \n",
        "    # 5th layer group\n",
        "    model.add(Conv3D(512, (3, 3, 3), activation='relu',\n",
        "                     padding='same', name='conv5a'))\n",
        "    model.add(Conv3D(512, (3, 3, 3), activation='relu',\n",
        "                     padding='same', name='conv5b'))\n",
        "    model.add(ZeroPadding3D(padding=((0, 0), (0, 1), (0, 1)), name='zeropad5'))\n",
        "    model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2),\n",
        "                           padding='valid', name='pool5'))\n",
        "    model.add(Flatten(name='flat5'))\n",
        "    # FC layers group\n",
        "    model.add(Dense(4096, activation='relu', name='fc6',input_dim=input_shape))\n",
        "    model.add(Dropout(.6))\n",
        "    model.add(Dense(4096, activation='relu', name='fc7'))\n",
        "    model.add(Dropout(.6))\n",
        "    model.add(Dense(487, activation='relu', name='fc7_2'))\n",
        "    model.add(Dropout(.5))\n",
        "    model.add(Dense(2, activation='softmax', name='fc8'))\n",
        "    return model\n",
        "\n",
        "# fcn_nt=fcn()\n",
        "# fcn_nt.summary()\n",
        "######################################################################################################################\n",
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "y_integers = np.argmax(Yntrain, axis=1)\n",
        "class_weights_n = compute_class_weight('balanced', np.unique(y_integers), y_integers)\n",
        "d_class_weights_n = dict(enumerate(class_weights_n))\n",
        "a = d_class_weights_n[0]; a=3*a\n",
        "d_class_weights_n[0]=a\n",
        "type(d_class_weights_n[1])\n",
        "print(d_class_weights_n)\n",
        "\n",
        "y_integers = np.argmax(Yotrain, axis=1)\n",
        "class_weights_o = compute_class_weight('balanced', np.unique(y_integers), y_integers)\n",
        "d_class_weights_o = dict(enumerate(class_weights_o))\n",
        "a = d_class_weights_o[0]; a=3*a\n",
        "d_class_weights_o[0]=a\n",
        "type(d_class_weights_o[1])\n",
        "print(d_class_weights_o)\n",
        "##########################################################################################################\n",
        "epochs=1\n",
        "n=len(Xntrain)\n",
        "o=len(Xotrain)\n",
        "ratio=(n/o)\n",
        "lamda=0.805\n",
        "\n",
        "\n",
        "batch_size=8192\n",
        "momentom=0.937\n",
        "i_iteration=50\n",
        "lr=0.01\n",
        "##########################################################################################################\n",
        "output=[]\n",
        "for gam in range(40,41,1):\n",
        "    lamda=gam/100\n",
        "    rrr=lamda*ratio\n",
        "    sgd = SGD(lr=lr, momentum=momentom, nesterov=True)\n",
        "    optimizer =Adam(lr=lr,beta_1=momentom) \n",
        "    fcn_nt=fcn()\n",
        "    fcn_nt.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "    #########################################################################################################################3\n",
        "\n",
        "    from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "    checkpoint = ModelCheckpoint(\"/content/drive/MyDrive/biased_weights_10.hdf5\", monitor='val_loss',\n",
        "                                verbose=1, save_best_only=True, save_weights_only=True,\n",
        "                                mode='auto', period=1)\n",
        "    early = EarlyStopping(monitor='val_loss', min_delta=0,\n",
        "                          patience=100, verbose=1, mode='auto',restore_best_weights=True)\n",
        "    ########################repeat two steps for number of iteration times\n",
        "    result=[]\n",
        "    for i in range(i_iteration):\n",
        "        print(\"number of iteration is:    \",i)\n",
        "        ###############################first train the last layer with normal loss\n",
        "        learning_rate=lr\n",
        "        for layer in fcn_nt.layers[:]:\n",
        "            layer.trainable = True\n",
        "        \n",
        "        history = fcn_nt.fit(x=Xntrain, y=Yntrain,\n",
        "                            validation_data=(Xvalid, Yvalid),\n",
        "                            class_weight = d_class_weights_n,\n",
        "                            batch_size=batch_size,\n",
        "                            epochs=epochs,\n",
        "                             verbose=1,callbacks=[checkpoint,early])     #[checkpoint,early] [early]  \n",
        "        \n",
        "        ################################# now train the rest of the network with total loss\n",
        "        learning_rate=rrr*lr                  #changing lr to reduce the loss of occluded data\n",
        "        # fcn_nt.layers[-4].trainable = False\n",
        "        # fcn_nt.layers[-3].trainable = False\n",
        "        # fcn_nt.layers[-2].trainable = False\n",
        "        # fcn_nt.layers[-1].trainable = False\n",
        "        history = fcn_nt.fit(x=Xotrain, y=Yotrain,\n",
        "                            validation_data=(Xvalid, Yvalid),\n",
        "                            class_weight = d_class_weights_o,\n",
        "                            batch_size=batch_size,\n",
        "                            epochs=epochs,\n",
        "                             verbose=1,callbacks=[checkpoint,early])     #[checkpoint,early] [early]                      \n",
        "        accuracy=history.history['accuracy']; val_accuracy=history.history['val_accuracy']\n",
        "        loss=history.history['loss']; val_loss=history.history['val_loss']\n",
        "        result.append([accuracy,val_accuracy,loss,val_loss])\n",
        "    result=np.array(result)\n",
        "    val_loss=min(result[:,-1])[0]\n",
        "    output.append([val_loss,lamda])\n",
        "#########################################################################################here we calculate test loss and accuracy\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "score = fcn_nt.evaluate(Xtest, Ytest, verbose=False) \n",
        "fcn_nt.metrics_names\n",
        "print('Test loss: ', score[0])    #Loss on test\n",
        "print('Test accuracy: ', score[1])\n",
        "\n",
        "########################################################here we claculate classification report and confusion matrix of test data\n",
        "Ytest_pred=ar(fcn_nt.predict(Xtest))\n",
        "targ=[]\n",
        "for i in range(len(Ytest_pred)):\n",
        "  if Ytest_pred[i,0]>Ytest_pred[i,1]:\n",
        "    targ+=[1]\n",
        "  else:\n",
        "    targ+=[0]\n",
        "Ytest=ar(Ytest)\n",
        "Ytest2=Ytest[:,0]\n",
        "\n",
        "print(\"classification_report:\\n\",classification_report(Ytest2, targ, labels=[0,1]))\n",
        "print(\"confusion_matrix:\\n\",confusion_matrix(Ytest2, targ))\n",
        "# list all data in history\n",
        "# print(result.history.keys())\n",
        "# summarize history for accuracy\n",
        "result=np.array(result)\n",
        "result=np.reshape(result,(-1,4))\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(result[:,0])\n",
        "plt.plot(result[:,1])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(result[:,2])\n",
        "plt.plot(result[:,3])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: 13.645222929936306, 1: 0.5617526746381372}\n",
            "{0: 13.647316992381583, 1: 0.5617420291633434}\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "number of iteration is:     0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "3/3 [==============================] - 4s 916ms/step - loss: 1.3868 - accuracy: 0.4722 - val_loss: 0.7173 - val_accuracy: 0.1142\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.71733, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "7/7 [==============================] - 5s 732ms/step - loss: 1.2934 - accuracy: 0.1247 - val_loss: 0.8650 - val_accuracy: 0.1142\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.71733\n",
            "number of iteration is:     1\n",
            "3/3 [==============================] - 2s 846ms/step - loss: 1.1798 - accuracy: 0.1099 - val_loss: 0.9642 - val_accuracy: 0.1142\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.71733\n",
            "7/7 [==============================] - 5s 731ms/step - loss: 1.0935 - accuracy: 0.1099 - val_loss: 1.2495 - val_accuracy: 0.1142\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.71733\n",
            "number of iteration is:     2\n",
            "3/3 [==============================] - 3s 864ms/step - loss: 1.0835 - accuracy: 0.1099 - val_loss: 1.3484 - val_accuracy: 0.1142\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.71733\n",
            "7/7 [==============================] - 5s 746ms/step - loss: 1.0659 - accuracy: 0.1099 - val_loss: 1.3689 - val_accuracy: 0.1142\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.71733\n",
            "number of iteration is:     3\n",
            "3/3 [==============================] - 3s 885ms/step - loss: 1.0636 - accuracy: 0.1099 - val_loss: 1.3070 - val_accuracy: 0.1142\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.71733\n",
            "7/7 [==============================] - 5s 751ms/step - loss: 1.0059 - accuracy: 0.1099 - val_loss: 1.1336 - val_accuracy: 0.1142\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.71733\n",
            "number of iteration is:     4\n",
            "3/3 [==============================] - 3s 883ms/step - loss: 1.0065 - accuracy: 0.1099 - val_loss: 1.0811 - val_accuracy: 0.1142\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.71733\n",
            "7/7 [==============================] - 5s 753ms/step - loss: 0.9491 - accuracy: 0.1099 - val_loss: 1.0089 - val_accuracy: 0.1142\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.71733\n",
            "number of iteration is:     5\n",
            "3/3 [==============================] - 3s 868ms/step - loss: 0.9628 - accuracy: 0.1099 - val_loss: 1.0025 - val_accuracy: 0.1142\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.71733\n",
            "7/7 [==============================] - 5s 731ms/step - loss: 0.8873 - accuracy: 0.1099 - val_loss: 0.9991 - val_accuracy: 0.1142\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.71733\n",
            "number of iteration is:     6\n",
            "3/3 [==============================] - 2s 846ms/step - loss: 0.8984 - accuracy: 0.1099 - val_loss: 1.0025 - val_accuracy: 0.1142\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.71733\n",
            "7/7 [==============================] - 5s 720ms/step - loss: 0.8215 - accuracy: 0.1099 - val_loss: 0.9597 - val_accuracy: 0.1142\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.71733\n",
            "number of iteration is:     7\n",
            "3/3 [==============================] - 2s 830ms/step - loss: 0.8267 - accuracy: 0.1099 - val_loss: 0.9397 - val_accuracy: 0.1142\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.71733\n",
            "7/7 [==============================] - 5s 710ms/step - loss: 0.7473 - accuracy: 0.1099 - val_loss: 0.8577 - val_accuracy: 0.1142\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.71733\n",
            "number of iteration is:     8\n",
            "3/3 [==============================] - 2s 847ms/step - loss: 0.7513 - accuracy: 0.1103 - val_loss: 0.8414 - val_accuracy: 0.3719\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.71733\n",
            "7/7 [==============================] - 5s 718ms/step - loss: 0.6578 - accuracy: 0.2997 - val_loss: 0.7634 - val_accuracy: 0.6427\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.71733\n",
            "number of iteration is:     9\n",
            "3/3 [==============================] - 2s 838ms/step - loss: 0.6641 - accuracy: 0.4335 - val_loss: 0.7490 - val_accuracy: 0.6566\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.71733\n",
            "7/7 [==============================] - 5s 743ms/step - loss: 0.5635 - accuracy: 0.5463 - val_loss: 0.6534 - val_accuracy: 0.7702\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.71733 to 0.65345, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "number of iteration is:     10\n",
            "3/3 [==============================] - 3s 871ms/step - loss: 0.5730 - accuracy: 0.6437 - val_loss: 0.6379 - val_accuracy: 0.7869\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.65345 to 0.63788, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "7/7 [==============================] - 5s 754ms/step - loss: 0.4728 - accuracy: 0.7030 - val_loss: 0.5579 - val_accuracy: 0.8716\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.63788 to 0.55787, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "number of iteration is:     11\n",
            "3/3 [==============================] - 3s 875ms/step - loss: 0.4831 - accuracy: 0.7992 - val_loss: 0.5433 - val_accuracy: 0.8736\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.55787 to 0.54334, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "7/7 [==============================] - 5s 746ms/step - loss: 0.3944 - accuracy: 0.8308 - val_loss: 0.4692 - val_accuracy: 0.9100\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.54334 to 0.46915, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "number of iteration is:     12\n",
            "3/3 [==============================] - 2s 867ms/step - loss: 0.4019 - accuracy: 0.8762 - val_loss: 0.4508 - val_accuracy: 0.9114\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.46915 to 0.45085, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "7/7 [==============================] - 5s 739ms/step - loss: 0.3241 - accuracy: 0.8952 - val_loss: 0.3724 - val_accuracy: 0.9346\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.45085 to 0.37242, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "number of iteration is:     13\n",
            "3/3 [==============================] - 2s 858ms/step - loss: 0.3371 - accuracy: 0.9065 - val_loss: 0.3500 - val_accuracy: 0.9367\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.37242 to 0.35000, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "7/7 [==============================] - 5s 737ms/step - loss: 0.2589 - accuracy: 0.9309 - val_loss: 0.2810 - val_accuracy: 0.9503\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.35000 to 0.28104, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "number of iteration is:     14\n",
            "3/3 [==============================] - 2s 850ms/step - loss: 0.2800 - accuracy: 0.9193 - val_loss: 0.2594 - val_accuracy: 0.9519\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.28104 to 0.25936, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "7/7 [==============================] - 5s 726ms/step - loss: 0.2090 - accuracy: 0.9501 - val_loss: 0.2096 - val_accuracy: 0.9596\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.25936 to 0.20956, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "number of iteration is:     15\n",
            "3/3 [==============================] - 3s 888ms/step - loss: 0.2364 - accuracy: 0.9293 - val_loss: 0.1979 - val_accuracy: 0.9594\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.20956 to 0.19787, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "7/7 [==============================] - 5s 738ms/step - loss: 0.1744 - accuracy: 0.9588 - val_loss: 0.1665 - val_accuracy: 0.9651\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.19787 to 0.16653, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "number of iteration is:     16\n",
            "3/3 [==============================] - 3s 874ms/step - loss: 0.2085 - accuracy: 0.9347 - val_loss: 0.1598 - val_accuracy: 0.9656\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.16653 to 0.15976, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "7/7 [==============================] - 5s 756ms/step - loss: 0.1527 - accuracy: 0.9621 - val_loss: 0.1390 - val_accuracy: 0.9687\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.15976 to 0.13896, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "number of iteration is:     17\n",
            "3/3 [==============================] - 3s 871ms/step - loss: 0.1894 - accuracy: 0.9399 - val_loss: 0.1345 - val_accuracy: 0.9693\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.13896 to 0.13446, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "7/7 [==============================] - 5s 744ms/step - loss: 0.1371 - accuracy: 0.9660 - val_loss: 0.1234 - val_accuracy: 0.9720\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.13446 to 0.12336, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "number of iteration is:     18\n",
            "3/3 [==============================] - 2s 857ms/step - loss: 0.1717 - accuracy: 0.9458 - val_loss: 0.1222 - val_accuracy: 0.9712\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.12336 to 0.12217, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "7/7 [==============================] - 5s 728ms/step - loss: 0.1274 - accuracy: 0.9665 - val_loss: 0.1148 - val_accuracy: 0.9723\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.12217 to 0.11484, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "number of iteration is:     19\n",
            "3/3 [==============================] - 2s 870ms/step - loss: 0.1624 - accuracy: 0.9481 - val_loss: 0.1164 - val_accuracy: 0.9716\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.11484\n",
            "7/7 [==============================] - 5s 739ms/step - loss: 0.1169 - accuracy: 0.9692 - val_loss: 0.1042 - val_accuracy: 0.9746\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.11484 to 0.10424, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "number of iteration is:     20\n",
            "3/3 [==============================] - 2s 855ms/step - loss: 0.1440 - accuracy: 0.9541 - val_loss: 0.1063 - val_accuracy: 0.9738\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.10424\n",
            "7/7 [==============================] - 5s 730ms/step - loss: 0.1112 - accuracy: 0.9693 - val_loss: 0.0995 - val_accuracy: 0.9754\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.10424 to 0.09955, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "number of iteration is:     21\n",
            "3/3 [==============================] - 2s 849ms/step - loss: 0.1404 - accuracy: 0.9558 - val_loss: 0.0979 - val_accuracy: 0.9753\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.09955 to 0.09792, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "7/7 [==============================] - 5s 729ms/step - loss: 0.1000 - accuracy: 0.9715 - val_loss: 0.0898 - val_accuracy: 0.9774\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.09792 to 0.08983, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "number of iteration is:     22\n",
            "3/3 [==============================] - 2s 860ms/step - loss: 0.1272 - accuracy: 0.9591 - val_loss: 0.0879 - val_accuracy: 0.9778\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.08983 to 0.08794, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "7/7 [==============================] - 5s 737ms/step - loss: 0.0932 - accuracy: 0.9730 - val_loss: 0.0894 - val_accuracy: 0.9771\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.08794\n",
            "number of iteration is:     23\n",
            "3/3 [==============================] - 3s 865ms/step - loss: 0.1212 - accuracy: 0.9590 - val_loss: 0.0821 - val_accuracy: 0.9789\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.08794 to 0.08209, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "7/7 [==============================] - 5s 736ms/step - loss: 0.0854 - accuracy: 0.9764 - val_loss: 0.0796 - val_accuracy: 0.9805\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.08209 to 0.07958, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "number of iteration is:     24\n",
            "3/3 [==============================] - 2s 856ms/step - loss: 0.1172 - accuracy: 0.9620 - val_loss: 0.0811 - val_accuracy: 0.9800\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.07958\n",
            "7/7 [==============================] - 5s 746ms/step - loss: 0.0809 - accuracy: 0.9763 - val_loss: 0.0765 - val_accuracy: 0.9814\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.07958 to 0.07652, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "number of iteration is:     25\n",
            "3/3 [==============================] - 3s 860ms/step - loss: 0.1021 - accuracy: 0.9679 - val_loss: 0.0750 - val_accuracy: 0.9818\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.07652 to 0.07500, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "7/7 [==============================] - 5s 752ms/step - loss: 0.0755 - accuracy: 0.9770 - val_loss: 0.0711 - val_accuracy: 0.9821\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.07500 to 0.07113, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "number of iteration is:     26\n",
            "3/3 [==============================] - 2s 864ms/step - loss: 0.1002 - accuracy: 0.9674 - val_loss: 0.0712 - val_accuracy: 0.9822\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.07113\n",
            "7/7 [==============================] - 5s 732ms/step - loss: 0.0700 - accuracy: 0.9779 - val_loss: 0.0683 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.07113 to 0.06827, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "number of iteration is:     27\n",
            "3/3 [==============================] - 2s 841ms/step - loss: 0.0938 - accuracy: 0.9678 - val_loss: 0.0646 - val_accuracy: 0.9847\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.06827 to 0.06457, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "7/7 [==============================] - 5s 725ms/step - loss: 0.0634 - accuracy: 0.9805 - val_loss: 0.0612 - val_accuracy: 0.9855\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.06457 to 0.06123, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "number of iteration is:     28\n",
            "3/3 [==============================] - 2s 854ms/step - loss: 0.0885 - accuracy: 0.9716 - val_loss: 0.0606 - val_accuracy: 0.9856\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.06123 to 0.06062, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "7/7 [==============================] - 5s 726ms/step - loss: 0.0619 - accuracy: 0.9812 - val_loss: 0.0601 - val_accuracy: 0.9857\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.06062 to 0.06014, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "number of iteration is:     29\n",
            "3/3 [==============================] - 3s 861ms/step - loss: 0.0854 - accuracy: 0.9708 - val_loss: 0.0581 - val_accuracy: 0.9860\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.06014 to 0.05807, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "7/7 [==============================] - 5s 743ms/step - loss: 0.0574 - accuracy: 0.9823 - val_loss: 0.0535 - val_accuracy: 0.9872\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.05807 to 0.05355, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "number of iteration is:     30\n",
            "3/3 [==============================] - 3s 875ms/step - loss: 0.0788 - accuracy: 0.9763 - val_loss: 0.0553 - val_accuracy: 0.9865\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.05355\n",
            "7/7 [==============================] - 5s 747ms/step - loss: 0.0537 - accuracy: 0.9827 - val_loss: 0.0540 - val_accuracy: 0.9866\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.05355\n",
            "number of iteration is:     31\n",
            "3/3 [==============================] - 2s 868ms/step - loss: 0.0782 - accuracy: 0.9748 - val_loss: 0.0539 - val_accuracy: 0.9866\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.05355\n",
            "7/7 [==============================] - 5s 736ms/step - loss: 0.0510 - accuracy: 0.9842 - val_loss: 0.0482 - val_accuracy: 0.9884\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.05355 to 0.04816, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "number of iteration is:     32\n",
            "3/3 [==============================] - 2s 856ms/step - loss: 0.0711 - accuracy: 0.9767 - val_loss: 0.0503 - val_accuracy: 0.9877\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.04816\n",
            "7/7 [==============================] - 5s 738ms/step - loss: 0.0492 - accuracy: 0.9843 - val_loss: 0.0494 - val_accuracy: 0.9879\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.04816\n",
            "number of iteration is:     33\n",
            "3/3 [==============================] - 2s 845ms/step - loss: 0.0704 - accuracy: 0.9762 - val_loss: 0.0451 - val_accuracy: 0.9888\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.04816 to 0.04509, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "7/7 [==============================] - 5s 727ms/step - loss: 0.0459 - accuracy: 0.9867 - val_loss: 0.0476 - val_accuracy: 0.9883\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.04509\n",
            "number of iteration is:     34\n",
            "3/3 [==============================] - 2s 840ms/step - loss: 0.0652 - accuracy: 0.9756 - val_loss: 0.0469 - val_accuracy: 0.9886\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.04509\n",
            "7/7 [==============================] - 5s 727ms/step - loss: 0.0442 - accuracy: 0.9865 - val_loss: 0.0433 - val_accuracy: 0.9888\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.04509 to 0.04329, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "number of iteration is:     35\n",
            "3/3 [==============================] - 2s 850ms/step - loss: 0.0624 - accuracy: 0.9800 - val_loss: 0.0467 - val_accuracy: 0.9883\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.04329\n",
            "7/7 [==============================] - 5s 735ms/step - loss: 0.0426 - accuracy: 0.9852 - val_loss: 0.0418 - val_accuracy: 0.9894\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.04329 to 0.04185, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "number of iteration is:     36\n",
            "3/3 [==============================] - 2s 855ms/step - loss: 0.0591 - accuracy: 0.9805 - val_loss: 0.0407 - val_accuracy: 0.9897\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.04185 to 0.04067, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "7/7 [==============================] - 5s 740ms/step - loss: 0.0408 - accuracy: 0.9873 - val_loss: 0.0395 - val_accuracy: 0.9901\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.04067 to 0.03952, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "number of iteration is:     37\n",
            "3/3 [==============================] - 3s 873ms/step - loss: 0.0585 - accuracy: 0.9805 - val_loss: 0.0402 - val_accuracy: 0.9898\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.03952\n",
            "7/7 [==============================] - 5s 756ms/step - loss: 0.0382 - accuracy: 0.9874 - val_loss: 0.0391 - val_accuracy: 0.9903\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.03952 to 0.03913, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "number of iteration is:     38\n",
            "3/3 [==============================] - 3s 878ms/step - loss: 0.0550 - accuracy: 0.9805 - val_loss: 0.0366 - val_accuracy: 0.9912\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.03913 to 0.03656, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "7/7 [==============================] - 5s 754ms/step - loss: 0.0357 - accuracy: 0.9889 - val_loss: 0.0356 - val_accuracy: 0.9916\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.03656 to 0.03557, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "number of iteration is:     39\n",
            "3/3 [==============================] - 2s 845ms/step - loss: 0.0595 - accuracy: 0.9810 - val_loss: 0.0373 - val_accuracy: 0.9907\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.03557\n",
            "7/7 [==============================] - 5s 732ms/step - loss: 0.0352 - accuracy: 0.9883 - val_loss: 0.0355 - val_accuracy: 0.9914\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.03557 to 0.03552, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "number of iteration is:     40\n",
            "3/3 [==============================] - 2s 851ms/step - loss: 0.0504 - accuracy: 0.9823 - val_loss: 0.0339 - val_accuracy: 0.9918\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.03552 to 0.03394, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "7/7 [==============================] - 5s 717ms/step - loss: 0.0345 - accuracy: 0.9898 - val_loss: 0.0355 - val_accuracy: 0.9914\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.03394\n",
            "number of iteration is:     41\n",
            "3/3 [==============================] - 2s 838ms/step - loss: 0.0526 - accuracy: 0.9805 - val_loss: 0.0359 - val_accuracy: 0.9912\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.03394\n",
            "7/7 [==============================] - 5s 719ms/step - loss: 0.0320 - accuracy: 0.9895 - val_loss: 0.0321 - val_accuracy: 0.9922\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.03394 to 0.03214, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "number of iteration is:     42\n",
            "3/3 [==============================] - 2s 849ms/step - loss: 0.0479 - accuracy: 0.9846 - val_loss: 0.0317 - val_accuracy: 0.9923\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.03214 to 0.03169, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "7/7 [==============================] - 5s 731ms/step - loss: 0.0324 - accuracy: 0.9900 - val_loss: 0.0322 - val_accuracy: 0.9923\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.03169\n",
            "number of iteration is:     43\n",
            "3/3 [==============================] - 3s 859ms/step - loss: 0.0511 - accuracy: 0.9836 - val_loss: 0.0335 - val_accuracy: 0.9920\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.03169\n",
            "7/7 [==============================] - 5s 724ms/step - loss: 0.0306 - accuracy: 0.9900 - val_loss: 0.0311 - val_accuracy: 0.9925\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.03169 to 0.03106, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "number of iteration is:     44\n",
            "3/3 [==============================] - 3s 869ms/step - loss: 0.0484 - accuracy: 0.9818 - val_loss: 0.0301 - val_accuracy: 0.9929\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.03106 to 0.03013, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "7/7 [==============================] - 5s 747ms/step - loss: 0.0284 - accuracy: 0.9915 - val_loss: 0.0297 - val_accuracy: 0.9928\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.03013 to 0.02969, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "number of iteration is:     45\n",
            "3/3 [==============================] - 2s 852ms/step - loss: 0.0453 - accuracy: 0.9845 - val_loss: 0.0311 - val_accuracy: 0.9925\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.02969\n",
            "7/7 [==============================] - 5s 733ms/step - loss: 0.0283 - accuracy: 0.9908 - val_loss: 0.0284 - val_accuracy: 0.9932\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.02969 to 0.02837, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "number of iteration is:     46\n",
            "3/3 [==============================] - 3s 857ms/step - loss: 0.0463 - accuracy: 0.9850 - val_loss: 0.0290 - val_accuracy: 0.9930\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.02837\n",
            "7/7 [==============================] - 5s 737ms/step - loss: 0.0259 - accuracy: 0.9920 - val_loss: 0.0268 - val_accuracy: 0.9936\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.02837 to 0.02685, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "number of iteration is:     47\n",
            "3/3 [==============================] - 2s 855ms/step - loss: 0.0439 - accuracy: 0.9853 - val_loss: 0.0282 - val_accuracy: 0.9933\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.02685\n",
            "7/7 [==============================] - 5s 738ms/step - loss: 0.0258 - accuracy: 0.9915 - val_loss: 0.0273 - val_accuracy: 0.9935\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.02685\n",
            "number of iteration is:     48\n",
            "3/3 [==============================] - 3s 908ms/step - loss: 0.0417 - accuracy: 0.9862 - val_loss: 0.0275 - val_accuracy: 0.9935\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.02685\n",
            "7/7 [==============================] - 5s 727ms/step - loss: 0.0245 - accuracy: 0.9922 - val_loss: 0.0271 - val_accuracy: 0.9937\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.02685\n",
            "number of iteration is:     49\n",
            "3/3 [==============================] - 2s 848ms/step - loss: 0.0379 - accuracy: 0.9859 - val_loss: 0.0245 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.02685 to 0.02449, saving model to /content/drive/MyDrive/biased_weights_10.hdf5\n",
            "7/7 [==============================] - 5s 721ms/step - loss: 0.0247 - accuracy: 0.9934 - val_loss: 0.0264 - val_accuracy: 0.9936\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.02449\n",
            "Test loss:  2.534980297088623\n",
            "Test accuracy:  0.5599667429924011\n",
            "classification_report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.99      0.72      8868\n",
            "           1       0.14      0.00      0.01      6774\n",
            "\n",
            "    accuracy                           0.56     15642\n",
            "   macro avg       0.35      0.49      0.36     15642\n",
            "weighted avg       0.38      0.56      0.41     15642\n",
            "\n",
            "confusion_matrix:\n",
            " [[8737  131]\n",
            " [6752   22]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byk9DxTCl1XA"
      },
      "source": [
        "#getting final results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BljqpSqsl8WC"
      },
      "source": [
        "\n",
        "def fcn():\n",
        "    \"\"\" Creates model object with the sequential API:\n",
        "    https://keras.io/models/sequential/\n",
        "    \"\"\"\n",
        "\n",
        "    model = Sequential()\n",
        "    input_shape = 8192\n",
        "\n",
        "    # FC layers group\n",
        "    model.add(Dense(4096, activation='relu', name='fc6',input_dim=input_shape))\n",
        "    model.add(Dropout(.5))\n",
        "    model.add(Dense(4096, activation='relu', name='fc7'))\n",
        "    model.add(Dropout(.5))\n",
        "    model.add(Dense(487, activation='relu', name='fc7_2'))\n",
        "    model.add(Dropout(.05))\n",
        "    model.add(Dense(2, activation='softmax', name='fc8'))\n",
        "\n",
        "    return model\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from keras.layers.merge import concatenate\n",
        "\n",
        "\n",
        "def load_fcn(fcn):             # defining the trained FCN \n",
        "\n",
        "    model = fcn()\n",
        "    try:\n",
        "        model.load_weights(\"/content/drive/MyDrive/biased_weights_10.hdf5\")\n",
        "    except OSError as err:\n",
        "        print('Check path to the model weights\\' file!\\n\\n', err)\n",
        "    except :\n",
        "        print(\"errooooor\")\n",
        "    fcn = Model(inputs=model.input,\n",
        "                      outputs=[ model.get_layer('fc6').output,\n",
        "                               model.get_layer('fc7').output,\n",
        "                               model.get_layer('fc7_2').output,\n",
        "                               model.get_layer('fc8').output])\n",
        "\n",
        "    # fcn.summary()\n",
        "    del model\n",
        "    return fcn\n",
        "\n",
        "# del Xtrain,Xtest,Xvalid,Ytrain,Ytest,Yvalid\n",
        "# del Xntrain,Xntest,Xnvalid,Yntrain,Yntest,Ynvalid\n",
        "# del Xotrain,Xotest,Xovalid,Yotrain,Yotest,Yovalid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8quWyUypuB4"
      },
      "source": [
        "#lib 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqzgYctqp3LE"
      },
      "source": [
        "import numpy as np\n",
        "ar = np.array\n",
        "import cv2\n",
        "import pickle\n",
        "from sklearn import svm\n",
        "from scipy.signal import find_peaks\n",
        "import os; import glob\n",
        "from tqdm import tqdm  \n",
        "import re\n",
        "def atoi(text):\n",
        "    return int(text) if text.isdigit() else text\n",
        "def natural_keys(text):\n",
        "    return [ atoi(c) for c in re.split(r'(\\d+)', text) ]\n",
        "\n",
        "c3d_len = 16\n",
        "             \n",
        "class extraction():                 # in class ham kollan feature exraction va label gozari anjam mide\n",
        "    def __init__(self, dataset_name, options, label,ds_addr,data_addr):\n",
        "        self.ds_addr = ds_addr; \n",
        "        self.data_addr = data_addr; \n",
        "        self.dataset_name = dataset_name; \n",
        "        self.label = label\n",
        "        self.options = options\n",
        "        self.dsrate = options['downsample_rate']\n",
        "        self.stride = options['stride'] #2 # was 2 meaning 8 frame steps. # 2 when stride=4\n",
        "\n",
        "\n",
        "    def load(self):                                               #this function either load feature-all or extract it\n",
        "        if new_train==True:\n",
        "          self.extract()\n",
        "        else:\n",
        "          try:\n",
        "              filename =self.data_addr+ self.label\n",
        "              infile = open(filename,'rb')\n",
        "              self.features_all = pickle.load(infile)\n",
        "              filename =self.data_addr+ self.label+'_labels'\n",
        "              infile = open(filename,'rb')\n",
        "              self.labels = pickle.load(infile)            \n",
        "              infile.close()\n",
        "              print(self.label, \"loaded successfully\")\n",
        "          except:           \n",
        "            self.extract()\n",
        "                \n",
        "    def extract(self):\n",
        "        print(self.label, \"extracting ...\")\n",
        "        if self.dataset_name not in os.listdir(self.ds_addr):\n",
        "            print(\"error , folder with dataset_name was not found\\n\\n\")             \n",
        "        \n",
        "        CNN_FC5 = load_c3d()                                                                    # loading only the convolution part of the C3D network \n",
        "        features_all = {}; labels=[]\n",
        "        f = open(self.ds_addr+ self.dataset_name+'/'+self.dataset_name+'.txt', \"r+\")\n",
        "        annot = f.readlines()                                                                 # reading lines of each text file\n",
        "        f.close()\n",
        "        videos = glob.glob(self.ds_addr+ self.dataset_name+'/*.avi')                          #reading videos of the dataset folder\n",
        "        videos.sort(key=natural_keys)\n",
        "\n",
        "        for i,video in (enumerate(videos)):                           \n",
        "            # print(video, annot[i])\n",
        "            capture = cv2.VideoCapture( video )\n",
        "            start=int(annot[i].split()[0])\n",
        "            end=int(annot[i].split()[1])\n",
        "            f_all=int(annot[i].split()[2])\n",
        "            num_frame = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "            if f_all!=num_frame:\n",
        "                print(\"error, num frames\", num_frame,f_all)\n",
        "            if not capture.isOpened:\n",
        "                print('Unable to open: ' + inp)\n",
        "                exit(0)\n",
        "            \n",
        "            frame16= []\n",
        "            i=0\n",
        "            for i in range(0, num_frame):\n",
        "                # print(num_frame)\n",
        "                ret, frame = capture.read()\n",
        "                    \n",
        "                if i% self.dsrate==0:                                     # applying downsampling to the frames\n",
        "                    frame_resized=frame\n",
        "                    frame16 += [frame_resized]\n",
        "                if frame is None:\n",
        "                    break\n",
        "        #============================================\n",
        "            frame16 = ar(frame16)\n",
        "            c3dlength = 16\n",
        "            if len(frame16)<16 :\n",
        "                # print(\"error... not enough frames\")\n",
        "                continue\n",
        "            sample_number = (len(frame16)-c3dlength)// self.stride +1       # calculating the number of input segments      \n",
        "            \n",
        "            features = []; count=0\n",
        "            for i in range(sample_number):\n",
        "                sc=i*self.stride; ec=i*self.stride+16                       # applying stride to the frames\n",
        "                f = ext(frame16[sc:ec],CNN_FC5)\n",
        "                features += [f]\n",
        "                if start==0 and end==0:\n",
        "                  label=-1\n",
        "                elif (sc*self.dsrate)>=start and (ec*self.dsrate)<=end:\n",
        "                  label=1\n",
        "                  count+=1                 \n",
        "                else:\n",
        "                  label=-1     \n",
        "                labels+=[label]\n",
        "            # print(count)\n",
        "            features_all.update({video:features})\n",
        "\n",
        "        self.labels=(labels)\n",
        "        self.features_all = features_all               \n",
        "        filename =self.data_addr+ self.label\n",
        "        outfile = open(filename,'wb')\n",
        "        outfile_l=open(filename+'_labels','wb')\n",
        "        pickle.dump(features_all ,outfile)\n",
        "        pickle.dump(self.labels ,outfile_l)\n",
        "        outfile.close()\n",
        "        del CNN_FC5\n",
        "        \n",
        "    def load_layer(self,layer): # loads data of nth layer of C3D with labels.\n",
        "        layer_features=[];\n",
        "        for k,v in self.features_all.items():      \n",
        "            layer_features +=[clipdata[layer] for clipdata in v]\n",
        "        print(np.shape(np.array(layer_features)))\n",
        "        return np.array(layer_features),self.labels    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMvnAFLDqB4n"
      },
      "source": [
        "#cross sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1kliOUSqANg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be491624-dbed-420e-88dd-3461288f02ee"
      },
      "source": [
        "#this code tries to extract features using fcn of C3d and classify them with SVM\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA as sklearnPCA\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "ar = np.array\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "# ===========================\n",
        "new_train=False\n",
        "# new_train=True\n",
        "def ext(l,c3d):                 #this function first reshapes l into 16*112*112*3  then gives it to c3d to predict\n",
        "    frames16 = l[:16].reshape((1,16,112,112,3)).astype('int') -100 # .astype('int')*255\n",
        "    flt = c3d.predict(frames16)   \n",
        "    features_4layers=fcn2.predict(flt)\n",
        "    del flt\n",
        "    return features_4layers\n",
        "\n",
        "options={'downsample_rate':1 , 'background_sub':False, 'stride':2}\n",
        "# datasets = [[['train'],['test']]]\n",
        "# datasets = [[['mine','mine_fall'],['mine2']]]\n",
        "datasets=[['Home_01','Home_02','Office','Office2','ur_fall','ur_adl','Lecture_room','Coffee_room_01','mine','mine_fall','mine2']]\n",
        "\n",
        "layers = [0,1,2]\n",
        "gridCV = {\n",
        "    'C': [1,10,100],\n",
        "    'gamma': [0.00001],\n",
        "        }\n",
        "for folder in datasets:\n",
        "    layerscores = []        \n",
        "    for layer in layers:\n",
        "        print('\\n Layer {}'.format(layer))\n",
        "        fcn2=load_fcn(fcn)\n",
        "        Xntrain=[]; Xntest = []\n",
        "        Yntrain=[]; Yntest = []\n",
        "        Xotrain=[]; Xotest = []\n",
        "        Yotrain=[]; Yotest = []\n",
        "\n",
        "        options={'downsample_rate':1 , 'background_sub':False, 'stride':2}\n",
        "        ds_addr= '/content/drive/MyDrive/full_frame/cross_subject/original/'\n",
        "        data_addr = '/content/drive/MyDrive/full_frame/data/trained/biased_org/'\n",
        "        \n",
        "        for i,ds in enumerate(folder):\n",
        "            ds=extraction(ds, options , ds,ds_addr,data_addr)\n",
        "            ds.load()\n",
        "            x,y=ds.load_layer(layer)\n",
        "            print(x.shape)\n",
        "            Xntrain+=[x]; Yntrain+=y\n",
        "        Xntrain = np.concatenate(Xntrain, axis=0)[:,0,:]\n",
        "        Xntrain, Xnvalid, Yntrain,Ynvalid = train_test_split(Xntrain, Yntrain, test_size=0.15 )\n",
        "        Xntrain, Xntest, Yntrain,Yntest = train_test_split(Xntrain, Yntrain, test_size=0.2 )\n",
        "\n",
        "        options={'downsample_rate':1 , 'background_sub':False, 'stride':32}\n",
        "        ds_addr= '/content/drive/MyDrive/full_frame/cross_subject/openpose/'\n",
        "        data_addr = '/content/drive/MyDrive/full_frame/data/trained/biased_cnt/'\n",
        "        \n",
        "        for i,ds in enumerate(folder):\n",
        "            ds=extraction(ds, options , ds,ds_addr,data_addr)\n",
        "            ds.load()\n",
        "            x,y=ds.load_layer(layer)\n",
        "            print(x.shape)\n",
        "            Xotrain+=[x]; Yotrain+=y\n",
        "        Xotrain = np.concatenate(Xotrain, axis=0)[:,0,:]\n",
        "        Xotrain, Xovalid, Yotrain,Yovalid = train_test_split(Xotrain, Yotrain, test_size=0.15 )\n",
        "        Xotrain, Xotest, Yotrain,Yotest = train_test_split(Xntrain, Yntrain, test_size=0.2 )\n",
        "        ####################################################################\n",
        "\n",
        "        Xtrain=np.concatenate((Xntrain,Xotrain),axis=0);  Ytrain=np.concatenate((Yntrain,Yotrain),axis=0)\n",
        "        maximum=np.amax(Xtrain); Xntrain=Xntrain/maximum; Xotrain=Xotrain/maximum\n",
        "        Xtest=np.concatenate((Xntest,Xotest),axis=0);     Ytest=np.concatenate((Yntest,Yotest),axis=0)\n",
        "        maximum=np.amax(Xtest); Xntest=Xntest/maximum; Xotest=Xotest/maximum\n",
        "        Xvalid=np.concatenate((Xnvalid,Xovalid),axis=0);     Yvalid=np.concatenate((Ynvalid,Yovalid),axis=0)\n",
        "        maximum=np.amax(Xvalid); Xnvalid=Xnvalid/maximum; Xovalid=Xovalid/maximum\n",
        "        print(\"Xtrain is:\",np.shape(Xtrain),\"          Xvalid is:\",np.shape(Xvalid),\"          Xtest is:\",np.shape(Xtest))   \n",
        "        print(\"Ytrain is:\",np.shape(Ytrain),\"          Yvalid is:\",np.shape(Yvalid),\"          Ytest is:\",np.shape(Ytest))  \n",
        "###################################################################################################\n",
        "        o=len(Xotrain[:,0]);          n=len(Xntrain[:,0])\n",
        "        ratio=(n/o)\n",
        "        lamda=0.5\n",
        "        rrr=lamda*ratio\n",
        "        c1=np.ones((n)); c2=rrr*np.ones((o))        #giving each sample a weight to reduce occluded data loss\n",
        "        ccc=np.concatenate((c1,c2),axis=0)\n",
        "\n",
        "        from sklearn.utils.class_weight import compute_class_weight        #calculating class weights in train dataset\n",
        "        class_weights = compute_class_weight('balanced', np.unique(Ytrain), Ytrain)\n",
        "        d_class_weights = {-1:class_weights[0], 1: (4*class_weights[1])}\n",
        "        print(d_class_weights)\n",
        "###################################################################################################\n",
        "        del ds\n",
        "        pca=sklearnPCA(n_components=200)    #reducing feature number\n",
        "        X = np.concatenate([Xtrain, Xtest, Xvalid])\n",
        "        pca.fit(X)\n",
        "        Xtrain = pca.transform(Xtrain)\n",
        "        Xvalid = pca.transform(Xvalid)\n",
        "        Xtest = pca.transform(Xtest)        \n",
        "        del X; del pca\n",
        "\n",
        "        scorel=[]\n",
        "        param=[]\n",
        "        for c in gridCV['C']:         #finding the best parameters in SVM\n",
        "            for g in gridCV['gamma']:\n",
        "                svclassifier1 = SVC(kernel='rbf', C=c, gamma=g,class_weight=d_class_weights)\n",
        "                svclassifier1.fit(Xtrain,Ytrain,sample_weight=ccc)\n",
        "                Ypred = svclassifier1.predict(Xvalid)\n",
        "                falsealarm=0; total = len(Ypred); miss=0; alarm=0;\n",
        "                for i in range(len(Ypred)):\n",
        "                    if Ypred[i]==1 and Yvalid[i]==-1:\n",
        "                        falsealarm+=1                   \n",
        "                    if Ypred[i]==-1 and Yvalid[i]==1:\n",
        "                        miss+=1\n",
        "                    if Ypred[i]==1 and Yvalid[i]==1:\n",
        "                        alarm+=1                               \n",
        "                s = alarm/(miss+alarm)* (1-falsealarm/(total-alarm-miss))#svclassifier1.score(xvalid, yvalid)\n",
        "                scorel +=[s]\n",
        "                param  +=[[c,g,s]]\n",
        "\n",
        "        bestparam= param[np.argmax(scorel)]\n",
        "        print('best parameters in validation C {}   gamma {} score(sens*falarm {}'.format(bestparam[0], bestparam[1],bestparam[2]))\n",
        "\n",
        "        # finding the performance of the best parameters with test dataset\n",
        "        svclassifier1 = SVC(kernel='rbf', C=bestparam[0], gamma=bestparam[1],class_weight=d_class_weights)\n",
        "        # svclassifier1.fit(Xtrain,Ytrain,sample_weight=c)\n",
        "        svclassifier1.fit(np.append(Xtrain,Xvalid,axis=0),np.append(Ytrain,Yvalid,axis=0))\n",
        "        s = svclassifier1.score(Xtest, Ytest)\n",
        "        Ypred = svclassifier1.predict(Xtest)\n",
        "        falsealarm=0; total = len(Ypred); miss=0; alarm=0;\n",
        "        for i in range(len(Ypred)):\n",
        "            if Ypred[i]==1 and Ytest[i]==-1:\n",
        "                falsealarm+=1\n",
        "            if Ypred[i]==-1 and Ytest[i]==1:\n",
        "                miss+=1\n",
        "            if Ypred[i]==1 and Ytest[i]==1:\n",
        "                alarm+=1\n",
        "        print('\\n Layer {}'.format(layer))\n",
        "        print('tot', total, 'alarm', alarm, 'f alrm', falsealarm,'miss',miss, 'else', total-alarm-falsealarm-miss)\n",
        "        print('SVMscore{:.3} sens{:.3} falarm{:.3} '.format\\\n",
        "              (s, alarm/(miss+alarm), falsealarm/(total-alarm-miss)), 'c,g ', bestparam[:2])\n",
        "        layerscores += [[layer , alarm/(miss+alarm)*(1- falsealarm/(total-alarm-miss))]]     \n",
        "\n",
        "        print(\"classification_report:\\n\",classification_report(Ytest, Ypred, labels=[-1,1]))\n",
        "        print(\"confusion_matrix:\\n\",confusion_matrix(Ytest, Ypred))\n",
        "\n",
        "        new_train=False\n",
        "    n = np.argmax([l[1] for l in layerscores])            \n",
        "    print('\\nFinal Best Layer {}  sens*falarm {:.3f}'.format(layerscores[n][0], layerscores[n][1]))        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Layer 0   dataset:['mine', 'mine_fall', 'mine2']\n",
            "mine extracting ...\n",
            "(1521, 1, 4096)\n",
            "mine_fall extracting ...\n",
            "error, num frames 190 191\n",
            "error, num frames 175 179\n",
            "error, num frames 164 165\n",
            "error, num frames 142 143\n",
            "(1795, 1, 4096)\n",
            "mine2 extracting ...\n",
            "(1041, 1, 4096)\n",
            "best parameters in validation C 100   gamma 1e-05 score(sens*falarm 0.9857278490704725\n",
            "\n",
            " Layer 0   dataset:['mine', 'mine_fall', 'mine2']\n",
            "tot 872 alarm 397 f alrm 1 miss 7 else 467\n",
            "SVMscore0.991 sens0.983 falarm0.00214  c,g  [100, 1e-05]\n",
            "classification_report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.99      1.00      0.99       468\n",
            "           1       1.00      0.98      0.99       404\n",
            "\n",
            "    accuracy                           0.99       872\n",
            "   macro avg       0.99      0.99      0.99       872\n",
            "weighted avg       0.99      0.99      0.99       872\n",
            "\n",
            "confusion_matrix:\n",
            " [[467   1]\n",
            " [  7 397]]\n",
            "\n",
            " Layer 1   dataset:['mine', 'mine_fall', 'mine2']\n",
            "mine loaded successfully\n",
            "(1521, 1, 4096)\n",
            "mine_fall loaded successfully\n",
            "(1795, 1, 4096)\n",
            "mine2 loaded successfully\n",
            "(1041, 1, 4096)\n",
            "best parameters in validation C 100   gamma 1e-05 score(sens*falarm 0.9623353559271866\n",
            "\n",
            " Layer 1   dataset:['mine', 'mine_fall', 'mine2']\n",
            "tot 872 alarm 390 f alrm 0 miss 7 else 475\n",
            "SVMscore0.992 sens0.982 falarm0.0  c,g  [100, 1e-05]\n",
            "classification_report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.99      1.00      0.99       475\n",
            "           1       1.00      0.98      0.99       397\n",
            "\n",
            "    accuracy                           0.99       872\n",
            "   macro avg       0.99      0.99      0.99       872\n",
            "weighted avg       0.99      0.99      0.99       872\n",
            "\n",
            "confusion_matrix:\n",
            " [[475   0]\n",
            " [  7 390]]\n",
            "\n",
            " Layer 2   dataset:['mine', 'mine_fall', 'mine2']\n",
            "mine loaded successfully\n",
            "(1521, 1, 487)\n",
            "mine_fall loaded successfully\n",
            "(1795, 1, 487)\n",
            "mine2 loaded successfully\n",
            "(1041, 1, 487)\n",
            "best parameters in validation C 100   gamma 1e-05 score(sens*falarm 0.0\n",
            "\n",
            " Layer 2   dataset:['mine', 'mine_fall', 'mine2']\n",
            "tot 872 alarm 382 f alrm 485 miss 0 else 5\n",
            "SVMscore0.444 sens1.0 falarm0.99  c,g  [100, 1e-05]\n",
            "classification_report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "          -1       1.00      0.01      0.02       490\n",
            "           1       0.44      1.00      0.61       382\n",
            "\n",
            "    accuracy                           0.44       872\n",
            "   macro avg       0.72      0.51      0.32       872\n",
            "weighted avg       0.75      0.44      0.28       872\n",
            "\n",
            "confusion_matrix:\n",
            " [[  5 485]\n",
            " [  0 382]]\n",
            "\n",
            "Final Best Layer 1  sens*falarm 0.982\n",
            "\n",
            " Layer 0   dataset:['mine', 'mine_fall', 'mine2']\n",
            "mine extracting ...\n",
            "(1521, 1, 4096)\n",
            "mine_fall extracting ...\n",
            "error, num frames 190 191\n",
            "error, num frames 175 179\n",
            "error, num frames 164 165\n",
            "error, num frames 142 143\n",
            "(1795, 1, 4096)\n",
            "mine2 extracting ...\n",
            "(1041, 1, 4096)\n",
            "best parameters in validation C 100   gamma 1e-05 score(sens*falarm 0.9811544739575605\n",
            "\n",
            " Layer 0   dataset:['mine', 'mine_fall', 'mine2']\n",
            "tot 872 alarm 375 f alrm 3 miss 1 else 493\n",
            "SVMscore0.995 sens0.997 falarm0.00605  c,g  [100, 1e-05]\n",
            "classification_report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "          -1       1.00      0.99      1.00       496\n",
            "           1       0.99      1.00      0.99       376\n",
            "\n",
            "    accuracy                           1.00       872\n",
            "   macro avg       1.00      1.00      1.00       872\n",
            "weighted avg       1.00      1.00      1.00       872\n",
            "\n",
            "confusion_matrix:\n",
            " [[493   3]\n",
            " [  1 375]]\n",
            "\n",
            " Layer 1   dataset:['mine', 'mine_fall', 'mine2']\n",
            "mine loaded successfully\n",
            "(1521, 1, 4096)\n",
            "mine_fall loaded successfully\n",
            "(1795, 1, 4096)\n",
            "mine2 loaded successfully\n",
            "(1041, 1, 4096)\n",
            "best parameters in validation C 100   gamma 1e-05 score(sens*falarm 0.964544095665172\n",
            "\n",
            " Layer 1   dataset:['mine', 'mine_fall', 'mine2']\n",
            "tot 872 alarm 362 f alrm 4 miss 12 else 494\n",
            "SVMscore0.982 sens0.968 falarm0.00803  c,g  [100, 1e-05]\n",
            "classification_report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.98      0.99      0.98       498\n",
            "           1       0.99      0.97      0.98       374\n",
            "\n",
            "    accuracy                           0.98       872\n",
            "   macro avg       0.98      0.98      0.98       872\n",
            "weighted avg       0.98      0.98      0.98       872\n",
            "\n",
            "confusion_matrix:\n",
            " [[494   4]\n",
            " [ 12 362]]\n",
            "\n",
            " Layer 2   dataset:['mine', 'mine_fall', 'mine2']\n",
            "mine loaded successfully\n",
            "(1521, 1, 487)\n",
            "mine_fall loaded successfully\n",
            "(1795, 1, 487)\n",
            "mine2 loaded successfully\n",
            "(1041, 1, 487)\n",
            "best parameters in validation C 100   gamma 1e-05 score(sens*falarm 0.0\n",
            "\n",
            " Layer 2   dataset:['mine', 'mine_fall', 'mine2']\n",
            "tot 872 alarm 377 f alrm 495 miss 0 else 0\n",
            "SVMscore0.432 sens1.0 falarm1.0  c,g  [100, 1e-05]\n",
            "classification_report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.00      0.00      0.00       495\n",
            "           1       0.43      1.00      0.60       377\n",
            "\n",
            "    accuracy                           0.43       872\n",
            "   macro avg       0.22      0.50      0.30       872\n",
            "weighted avg       0.19      0.43      0.26       872\n",
            "\n",
            "confusion_matrix:\n",
            " [[  0 495]\n",
            " [  0 377]]\n",
            "\n",
            "Final Best Layer 0  sens*falarm 0.991\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}